{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_news_data = datasets.load_dataset('ag_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Choose transformer model\n",
    "TRANSFORMER_MODEL = \"distilbert-base-uncased\"\n",
    "\n",
    "# Init tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL)\n",
    "\n",
    "# Helper function to tokenize the input text\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", max_length=64, truncation=True)\n",
    "\n",
    "# Tokenize dataset\n",
    "data_tokenized = ag_news_data.map(tokenize, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set convenient output format\n",
    "data_tokenized.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from small_text.integrations.transformers import TransformersDataset\n",
    "from small_text.base import LABEL_UNLABELED\n",
    "\n",
    "\n",
    "# Create tuples from the tokenized training data\n",
    "data = [\n",
    "    # Need to add an extra dimension to indicate a batch size of 1 -> [None]\n",
    "    (row[\"input_ids\"][None], row[\"attention_mask\"][None], LABEL_UNLABELED)\n",
    "    for row in data_tokenized[\"train\"]\n",
    "]\n",
    "\n",
    "# Create the dataset for small-text\n",
    "dataset = TransformersDataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset\n",
    "data_test = [\n",
    "    (row[\"input_ids\"][None], row[\"attention_mask\"][None], int(row[\"label\"]))\n",
    "    for row in data_tokenized[\"test\"]\n",
    "]\n",
    "dataset_test = TransformersDataset(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from small_text.integrations.transformers.classifiers.factories import TransformerBasedClassificationFactory\n",
    "from small_text.integrations.transformers import TransformerModelArguments\n",
    "from small_text.query_strategies import BreakingTies\n",
    "from small_text.active_learner import PoolBasedActiveLearner\n",
    "\n",
    "\n",
    "# Define our classifier\n",
    "clf_factory = TransformerBasedClassificationFactory(\n",
    "    TransformerModelArguments(TRANSFORMER_MODEL),\n",
    "    num_classes=4,\n",
    "    # If you have a cuda device, specify it here.\n",
    "    # Otherwise, just remove the following line.\n",
    "    # kwargs={\"device\": \"cuda\"}\n",
    ")\n",
    "\n",
    "# Define our query strategy\n",
    "query_strategy = BreakingTies()\n",
    "\n",
    "# Use the active learner with a pool containing all unlabeled data\n",
    "active_learner = PoolBasedActiveLearner(clf_factory, query_strategy, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from small_text.initialization import random_initialization\n",
    "import numpy as np\n",
    "# Fix seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# Number of samples in our queried batches\n",
    "NUM_SAMPLES = 10\n",
    "\n",
    "# Randomly draw an initial subset from the data pool\n",
    "initial_indices = random_initialization(dataset, NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "\n",
    "rb.init(api_url=\"http://rubrix:80\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for the dataset\n",
    "DATASET_NAME = \"test_with_active_learning\"\n",
    "\n",
    "# Define labeling schema\n",
    "labels = ag_news_data[\"train\"].features[\"label\"].names\n",
    "settings = rb.TextClassificationSettings(label_schema=labels)\n",
    "\n",
    "# Create dataset with a label schema\n",
    "rb.configure_dataset(name=DATASET_NAME, settings=settings)\n",
    "\n",
    "# Create records from the initial batch\n",
    "records = [\n",
    "    rb.TextClassificationRecord(\n",
    "        text=ag_news_data[\"train\"][\"text\"][idx],\n",
    "        metadata={\"batch_id\": 0},\n",
    "        id=idx,\n",
    "    )\n",
    "    for idx in initial_indices\n",
    "]\n",
    "\n",
    "# Log initial records to Rubrix\n",
    "rb.log(records, DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubrix.listeners import listener\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define some helper variables\n",
    "LABEL2INT = ag_news_data[\"train\"].features[\"label\"].str2int\n",
    "ACCURACIES = []\n",
    "\n",
    "# Set up the active learning loop with the listener decorator\n",
    "@listener(\n",
    "    dataset=DATASET_NAME,\n",
    "    query=\"status:Validated AND metadata.batch_id:{batch_id}\",\n",
    "    condition=lambda search: search.total==NUM_SAMPLES,\n",
    "    execution_interval_in_seconds=3,\n",
    "    batch_id=0\n",
    ")\n",
    "def active_learning_loop(records, ctx):\n",
    "\n",
    "    # 1. Update active learner\n",
    "    print(f\"Updating with batch_id {ctx.query_params['batch_id']} ...\")\n",
    "    y = np.array([LABEL2INT(rec.annotation) for rec in records])\n",
    "\n",
    "    # initial update\n",
    "    if ctx.query_params[\"batch_id\"] == 0:\n",
    "        indices = np.array([rec.id for rec in records])\n",
    "        active_learner.initialize_data(indices, y)\n",
    "    # update with the prior queried indices\n",
    "    else:\n",
    "        active_learner.update(y)\n",
    "    print(\"Done!\")\n",
    "\n",
    "    # 2. Query active learner\n",
    "    print(\"Querying new data points ...\")\n",
    "    queried_indices = active_learner.query(num_samples=NUM_SAMPLES)\n",
    "    ctx.query_params[\"batch_id\"] += 1\n",
    "    new_records = [\n",
    "        rb.TextClassificationRecord(\n",
    "            text=ag_news_data[\"train\"][\"text\"][idx],\n",
    "            metadata={\"batch_id\": ctx.query_params[\"batch_id\"]},\n",
    "            id=idx,\n",
    "        )\n",
    "        for idx in queried_indices\n",
    "    ]\n",
    "\n",
    "    # 3. Log the batch to Rubrix\n",
    "    rb.log(new_records, DATASET_NAME)\n",
    "\n",
    "    # 4. Evaluate current classifier on the test set\n",
    "    print(\"Evaluating current classifier ...\")\n",
    "    accuracy = accuracy_score(\n",
    "        dataset_test.y,\n",
    "        active_learner.classifier.predict(dataset_test),\n",
    "    )\n",
    "    ACCURACIES.append(accuracy)\n",
    "    print(\"Done!\")\n",
    "\n",
    "    print(\"Waiting for annotations ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_learning_loop.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.Series(ACCURACIES).plot(xlabel=\"Iteration\", ylabel=\"Accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCURACIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_learning_loop.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "45566e35053653ddc5f64b81b73793e0e75c4a9436c09bd7e71671fcb8c38cef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
