{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active learning loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/src/.venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "\n",
    "# HF\n",
    "from datasets import load_dataset, Features, Value, ClassLabel, Version\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# small text\n",
    "from small_text import random_initialization\n",
    "from small_text.active_learner import PoolBasedActiveLearner\n",
    "from small_text.base import LABEL_UNLABELED\n",
    "from small_text.integrations.transformers import TransformersDataset, TransformerModelArguments\n",
    "from small_text.integrations.transformers.classifiers.factories import TransformerBasedClassificationFactory\n",
    "from small_text.query_strategies import BreakingTies\n",
    "\n",
    "# rubrix\n",
    "import rubrix as rb\n",
    "from rubrix.listeners import listener\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 07:59:06.449 | WARNING  | datasets.builder:_create_builder_config:473 - Using custom data configuration default\n",
      "2022-10-12 07:59:06.455 | WARNING  | datasets.builder:download_and_prepare:732 - Found cached dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n",
      "100%|██████████| 2/2 [00:00<00:00, 453.54it/s]\n"
     ]
    }
   ],
   "source": [
    "DATASET_AG_NEWS = \"bergr7/weakly_supervised_ag_news\"\n",
    "TRANSFORMER_MODEL = \"distilbert-base-uncased\"\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL)\n",
    "LABELS = load_dataset('ag_news')[\"train\"].features[\"label\"].names\n",
    "NUM_SAMPLES_ITER = 5\n",
    "NUM_CLASSES = len(LABELS)\n",
    "DATASET_NAME = \"active_learning_loop\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 07:53:31.675 | WARNING  | datasets.builder:_create_builder_config:473 - Using custom data configuration bergr7--weakly_supervised_ag_news-6f78f309523478bd\n",
      "2022-10-12 07:53:31.677 | WARNING  | datasets.builder:download_and_prepare:732 - Found cached dataset csv (/root/.cache/huggingface/datasets/bergr7___csv/bergr7--weakly_supervised_ag_news-6f78f309523478bd/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
      "100%|██████████| 3/3 [00:00<00:00, 965.76it/s]\n",
      "2022-10-12 07:53:31.782 | WARNING  | datasets.builder:_create_builder_config:473 - Using custom data configuration bergr7--weakly_supervised_ag_news-9442e7dc9bdd01c3\n",
      "2022-10-12 07:53:31.784 | WARNING  | datasets.builder:download_and_prepare:732 - Found cached dataset csv (/root/.cache/huggingface/datasets/bergr7___csv/bergr7--weakly_supervised_ag_news-9442e7dc9bdd01c3/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
      "100%|██████████| 1/1 [00:00<00:00, 926.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# labeled data - train.csv contains weak labels\n",
    "labeled_data_files = {\n",
    "    \"train\": \"train.csv\",\n",
    "    \"validation\": \"validation.csv\", \n",
    "    \"test\": \"test.csv\"\n",
    "}\n",
    "\n",
    "# unlabeled data - not covered by LFs\n",
    "unlabeled_data_files = {\"unlabeled\": \"unlabeled_train.csv\"}\n",
    "\n",
    "# Define schema\n",
    "labeled_features = Features(\n",
    "    {\n",
    "        \"text\": Value(\"string\"),\n",
    "        \"label\": ClassLabel(\n",
    "            num_classes=4,\n",
    "            names=['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "        )\n",
    "    }\n",
    ")\n",
    "unlabeled_features = Features({\"text\": Value(\"string\")})\n",
    "\n",
    "# load data\n",
    "labeled_dataset = load_dataset(\n",
    "    DATASET_AG_NEWS,\n",
    "    data_files=labeled_data_files,\n",
    "    features=labeled_features\n",
    ")\n",
    "\n",
    "unlabeled_dataset = load_dataset(\n",
    "    DATASET_AG_NEWS,\n",
    "    data_files=unlabeled_data_files,\n",
    "    features=unlabeled_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 37340\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 24000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 7600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    unlabeled: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 58660\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text\n",
    "weak_train_text = [row[\"text\"] for row in labeled_dataset[\"train\"]][:100]\n",
    "unlabeled_train_text = [row[\"text\"] for row in unlabeled_dataset[\"unlabeled\"]][:200]\n",
    "# labels\n",
    "weak_train_labels = [row[\"label\"] for row in labeled_dataset[\"train\"]][:100]\n",
    "unlabeled_no_labels = [LABEL_UNLABELED for _ in unlabeled_train_text][:200]\n",
    "# dataset\n",
    "train_text = np.array(weak_train_text + unlabeled_train_text)\n",
    "train_labels = np.array(weak_train_labels + unlabeled_no_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the dataset\n",
    "\n",
    "Wrap the dataset in a specific data class by small-text -> `TransformersDataset` since we are gonna use `distilbert-base-uncased`\n",
    "\n",
    "`TransformersDataset` should contain the tokenized input text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/src/.venv/lib/python3.10/site-packages/small_text/utils/annotations.py:67: ExperimentalWarning: The function from_arrays is experimental and maybe subject to change soon.\n",
      "  warnings.warn(f'The {subject} {func_or_class.__name__} is experimental '\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "dataset = TransformersDataset.from_arrays(\n",
    "    train_text,\n",
    "    train_labels,\n",
    "    tokenizer=TOKENIZER,\n",
    "    target_labels=[0, 1, 2, 3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# visualize an input - weak label\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput_ids: \u001b[39m\u001b[39m{\u001b[39;00mdataset\u001b[39m.\u001b[39mdata[\u001b[39m37339\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtoken_type_ids: \u001b[39m\u001b[39m{\u001b[39;00mdataset\u001b[39m.\u001b[39mdata[\u001b[39m37339\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlabel: \u001b[39m\u001b[39m{\u001b[39;00mdataset\u001b[39m.\u001b[39mdata[\u001b[39m37339\u001b[39m][\u001b[39m2\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# visualize an input - weak label\n",
    "print(f\"input_ids: {dataset.data[37339][0]}\")\n",
    "print(f\"token_type_ids: {dataset.data[37339][1]}\")\n",
    "print(f\"label: {dataset.data[37339][2]}\")\n",
    "\n",
    "# visualize an input - unlabeled\n",
    "print(f\"input_ids: {dataset.data[50000][0]}\")\n",
    "print(f\"token_type_ids: {dataset.data[50000][1]}\")\n",
    "print(f\"label: {dataset.data[50000][2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation dataset\n",
    "val_text = [row[\"text\"] for row in labeled_dataset[\"validation\"]][:20]\n",
    "val_labels = [row[\"label\"] for row in labeled_dataset[\"validation\"]][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = TransformersDataset.from_arrays(\n",
    "    val_text,\n",
    "    val_labels,\n",
    "    tokenizer=TOKENIZER,\n",
    "    target_labels=[0, 1, 2, 3]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Notes\n",
    "\n",
    "- cleaning??\n",
    "- test set???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up active learner\n",
    "\n",
    "- Component 1 -> classifier\n",
    "- Component 2 -> query strategy (sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-params\n",
    "params = dict(\n",
    "        {\n",
    "            \"lr\": 3e-5,\n",
    "            \"num_epochs\": 1,\n",
    "            \"mini_batch_size\": 32,\n",
    "            \"model_selection\": True,\n",
    "            \"device\": \"cuda\"\n",
    "        \n",
    "    }\n",
    ")\n",
    "\n",
    "# classifier\n",
    "model_factory = TransformerBasedClassificationFactory(\n",
    "    TransformerModelArguments(TRANSFORMER_MODEL),\n",
    "    num_classes=NUM_CLASSES,\n",
    "    kwargs=params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define query strategy\n",
    "query_strategy = BreakingTies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_active_learner(active_learner, train_labels, weak_train_labels):\n",
    "\n",
    "    indices_initial = [i for i in range(len(weak_train_labels))]\n",
    "    active_learner.initialize_data(indices_initial, train_labels[indices_initial])\n",
    "\n",
    "    return indices_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_learner = PoolBasedActiveLearner(\n",
    "    model_factory,\n",
    "    query_strategy,\n",
    "    dataset,\n",
    "    reuse_model=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes quite a bit of time... (1hr!!) - we initilize the active learner with the weak labels\n",
    "indices_labeled = initialize_active_learner(active_learner, train_labels, weak_train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most query strategies, including ours, require a trained model, we randomly draw a subset from the data pool to initialize our AL system. After obtaining the labels for this batch of instances, the active learner will use them to create the first classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning loop with Rubrix\n",
    "\n",
    "Configure a Rubrix dataset and set up active learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init rubrix\n",
    "\n",
    "RUBRIX_URL = os.getenv(\"RUBRIX_API_URL\", \"http://localhost:6900\")\n",
    "\n",
    "rb.init(api_url=RUBRIX_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = rb.TextClassificationSettings(label_schema=LABELS)\n",
    "# configure dataset\n",
    "rb.configure_dataset(name=DATASET_NAME, settings=settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 records logged to http://rubrix:80/datasets/rubrix/bg_test_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BulkResponse(dataset='bg_test_3', processed=5, failed=0)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first batch querying with active learner\n",
    "batch_1_indices = active_learner.query(num_samples=NUM_SAMPLES_ITER)\n",
    "\n",
    "records = [\n",
    "    rb.TextClassificationRecord(\n",
    "        text=train_text[idx],\n",
    "        metadata={\"batch_id\": 0},\n",
    "        id=idx,\n",
    "    )\n",
    "    for idx in batch_1_indices\n",
    "]\n",
    "\n",
    "# Log initial records to Rubrix\n",
    "rb.log(records, DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL2INT = labeled_dataset[\"train\"].features[\"label\"].str2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCURACIES = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "@listener(dataset=DATASET_NAME,\n",
    "    query=\"status:Validated AND metadata.batch_id:{batch_id}\",\n",
    "    condition=lambda search: search.total==NUM_SAMPLES_ITER,\n",
    "    execution_interval_in_seconds=3,\n",
    "    batch_id=0\n",
    ")\n",
    "def active_learning_loop(records, ctx):\n",
    "    \n",
    "    # 1. Update active learner\n",
    "    print(f\"Updating with batch_id {ctx.query_params['batch_id']} ...\")\n",
    "    y = np.array([LABEL2INT(rec.annotation) for rec in records])\n",
    "\n",
    "    # update with the prior queried indices\n",
    "    active_learner.update(y)\n",
    "    print(\"Done!\")\n",
    "\n",
    "    # 2. Query active learner\n",
    "    print(\"Querying new data points ...\")\n",
    "    queried_indices = active_learner.query(num_samples=NUM_SAMPLES_ITER)\n",
    "    new_batch = ctx.query_params[\"batch_id\"] + 1\n",
    "    new_records = [\n",
    "        rb.TextClassificationRecord(\n",
    "            text=train_text[idx],\n",
    "            metadata={\"batch_id\": new_batch},\n",
    "            id=idx,\n",
    "        )\n",
    "        for idx in queried_indices\n",
    "    ]\n",
    "\n",
    "    # 3. Log the batch to Rubrix\n",
    "    rb.log(new_records, DATASET_NAME)\n",
    "\n",
    "    # 4. Evaluate current classifier on the test set\n",
    "    print(\"Evaluating current classifier ...\")\n",
    "    accuracy = accuracy_score(\n",
    "        val_dataset.y,\n",
    "        active_learner.classifier.predict(val_dataset),\n",
    "    )\n",
    "\n",
    "    ACCURACIES.append(accuracy)\n",
    "    ctx.query_params[\"batch_id\"] = new_batch\n",
    "    print(\"Done!\")\n",
    "\n",
    "    print(\"Waiting for annotations ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start active learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating with batch_id 0 ...\n",
      "Done!\n",
      "Querying new data points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 records logged to http://rubrix:80/datasets/rubrix/bg_test_3\n",
      "Evaluating current classifier ...\n",
      "Done!\n",
      "Waiting for annotations ...\n",
      "Updating with batch_id 1 ...\n",
      "Done!\n",
      "Querying new data points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 60.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 records logged to http://rubrix:80/datasets/rubrix/bg_test_3\n",
      "Evaluating current classifier ...\n",
      "Done!\n",
      "Waiting for annotations ...\n",
      "Updating with batch_id 2 ...\n",
      "Done!\n",
      "Querying new data points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 records logged to http://rubrix:80/datasets/rubrix/bg_test_3\n",
      "Evaluating current classifier ...\n",
      "Done!\n",
      "Waiting for annotations ...\n"
     ]
    }
   ],
   "source": [
    "active_learning_loop.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_learning_loop.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c688ced32421b6e0c2623d0cc61f1e3db35c35392b047fa76651d72914615363"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
